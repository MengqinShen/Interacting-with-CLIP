# Interacting-with-CLIP
Given an unseen image, test if CLIP (Contrastive Language-Image Pre-training) model can identify its category.
The CLIP (Contrastive Language-Image Pre-training) model is a deep learning model developed by OpenAI that learns to associate images and text in a shared embedding space. This model is designed to understand images in a way that can be aligned with natural language descriptions, enabling a wide range of tasks that involve both visual and textual data.
